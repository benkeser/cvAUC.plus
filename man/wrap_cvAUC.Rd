% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mainFunctions.R
\name{wrap_cvAUC}
\alias{wrap_cvAUC}
\title{wrap_cvAUC}
\usage{
wrap_cvAUC(
  Y,
  X,
  learner,
  confidence = 0.95,
  seed = 1234,
  id = NULL,
  cvControl = list(V = 10L, stratifyCV = FALSE, shuffle = TRUE, validRows = NULL),
  returnFits = FALSE,
  parallel = FALSE,
  ...
)
}
\arguments{
\item{Y}{A \code{numeric} vector of class labels}

\item{X}{A \code{data.frame} of variables that \code{learner} will use
to predict. It is assumed that the format at code{X} will place nicely with
the function specified by \code{learner}.}

\item{learner}{A \code{character} name of a function that generates predictions.
The function should take as input \code{Y}, \code{X}, and \code{newX}, use \code{X}
to predict \code{Y} and return predictions for \code{newX}. See examples below.}

\item{confidence}{A \code{numeric} between 0 and 1 specifying the nominal coverage
probability for the confidence interval. Default is 0.95.}

\item{seed}{A \code{numeric} specifying what seed to set prior to data splitting. 
If \code{diff_cvAUC} is to be used afterwards to compare CV-AUCs for different fits, 
be sure to specify the same \code{seed} so that the sample splits are the same.}

\item{id}{A \code{numeric} vector of observation identifiers. Only used for splitting
data and should probably be ignored for now as the CV-AUC calculations do not account for
dependent data in any other way.}

\item{cvControl}{A \code{list} of a specific form. See \code{?SuperLearner.CV.control} for
more information.}

\item{returnFits}{A \code{boolean} indicating whether or not to return the model fit objects
for each fold.}

\item{parallel}{A \code{boolean} indicating whether to perform the model fitting across folds in 
parallel. If \code{TRUE} then \code{foreach} is used to parallelize the fitting.}

\item{...}{Not currently used}
}
\value{
An object of class \code{wrap_cvAUC} with the following entries: 
\item{cvAUC}{The estimated cross-validated AUC.}
\item{se}{The standard error for the estimated CV-AUC.}
\item{ci}{A \code{100*confidence} percent confidence interval.}
\item{confidence}{The level of confidence for the interval.}
\item{ic}{The estimated influence function evaluated on the observations.}
\item{folds}{The row indices for each validation sample.}
\item{fitLibrary}{The fit objects from \code{learner}.}
\item{learner}{The learner that was used to generate predictions.}
\item{p}{The one-sided p-value testing the null hypothesis that CV-AUC = 0.5 
against the alternative that CV-AUC > 0.5. }
}
\description{
This function is a helper wrapper function for the \code{cvAUC} function
included in the \code{cvAUC} package by Erin LeDell. The function allows
the user to use the data splitting options available in the \code{SuperLearner}
package and provides a specific structure for different learners to be used 
to generate predictions. The biggest addition with this function is that influence
functions are returned, which can be used to develop hypothesis tests comparing
the CV-AUC between two different learners. The function \code{diff_cvAUC} 
performs these tests.
}
\examples{
n <- 1000
X <- data.frame(x1=rnorm(n),x2=rnorm(n))
Y <- rbinom(n,1,plogis(X$x1 + X$x2))
myglm1 <- function(Y,X,newX){
   fm <- glm(Y~.,data=X,family=binomial())
   pred <- predict(fm,newdata=newX,type="response")
   return(list(fit = fm, pred = pred))
}
myglm2 <- function(Y,X,newX){
  fm <- glm(Y~x1,data=X,family=binomial())
  pred <- predict(fm,newdata=newX,type="response")
  return(list(fit = fm, pred = pred))
}
out1 <- wrap_cvAUC(Y = Y, X=X, learner = "myglm1")
out2 <- wrap_cvAUC(Y = Y, X=X, learner = "myglm2")
}
